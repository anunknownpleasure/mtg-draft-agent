{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/doctorsmylie/mtg-draft-agent/blob/main/model-trainer-with-deck-eval.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "532b0c61",
      "metadata": {
        "id": "532b0c61",
        "outputId": "bc410072-2b7b-448c-8123-cd700c7c324d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'mtg-draft-agent'...\n",
            "remote: Enumerating objects: 257, done.\u001b[K\n",
            "remote: Counting objects: 100% (39/39), done.\u001b[K\n",
            "remote: Compressing objects: 100% (32/32), done.\u001b[K\n",
            "remote: Total 257 (delta 16), reused 13 (delta 4), pack-reused 218 (from 1)\u001b[K\n",
            "Receiving objects: 100% (257/257), 13.82 MiB | 15.31 MiB/s, done.\n",
            "Resolving deltas: 100% (136/136), done.\n",
            "/content/mtg-draft-agent\n",
            "Starting config...\n",
            "Running in Colab? Yes\n",
            "\n",
            "Configuring Google Colab...\n",
            "Mounting Drive...\n",
            "Mounted at /content/mtg-draft-agent/drive\n",
            "BASE_PATH =  /content/mtg-draft-agent\n",
            "DATA_FOLDER = /content/mtg-draft-agent/drive/MyDrive/Erdos25/MTGdraft\n",
            "BASE_PATH == os.getcwd(): True\n",
            "\n",
            "Configuration done\n"
          ]
        }
      ],
      "source": [
        "# Configure Drive or Jupyter notebook -- only runs when first loaded\n",
        "if \"CONFIG_DONE\" not in globals():\n",
        "    # Need to mount drive and clone repo to access data and functions\n",
        "    try:\n",
        "        from google.colab import drive  # type: ignore\n",
        "\n",
        "        IN_COLAB = True\n",
        "\n",
        "        # clone repo\n",
        "        !git clone https://github.com/doctorsmylie/mtg-draft-agent\n",
        "        %cd mtg-draft-agent\n",
        "\n",
        "    except ModuleNotFoundError:\n",
        "        IN_COLAB = False\n",
        "\n",
        "    # Finish configuration -- also configures notebook outside of Colab\n",
        "    %run \"project_path.ipynb\"\n",
        "else:\n",
        "    print(\"Config done\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "8282d89c",
      "metadata": {
        "id": "8282d89c",
        "outputId": "d9c67be3-4536-423f-ccb6-cb99b7a06236",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# from datasets import Dataset\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
        "\n",
        "import pathlib\n",
        "from itertools import product\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from time import time\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "import functions.card_io as card_io\n",
        "import functions.utils as utils\n",
        "\n",
        "# Setting device on GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "a21fb76a",
      "metadata": {
        "id": "a21fb76a"
      },
      "outputs": [],
      "source": [
        "drafts_all = pd.read_parquet('clean_data/DSK_drafts.parquet')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "drafts = drafts_all.iloc[:42*100,:]"
      ],
      "metadata": {
        "id": "79qDSptQXg8B"
      },
      "id": "79qDSptQXg8B",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "09145659",
      "metadata": {
        "id": "09145659"
      },
      "outputs": [],
      "source": [
        "from bots.lstm_bot import *\n",
        "\n",
        "# Model, Loss and Optimizer\n",
        "vocab_size = 286\n",
        "\n",
        "embed_dim = 128 #64\n",
        "hidden_dim = 256 #128\n",
        "num_layers = 2\n",
        "p_LSTM = 0.3\n",
        "p_out = 0.5\n",
        "model = DraftBotLSTM(\n",
        "    vocab_size, embed_dim, hidden_dim, num_layers=num_layers, p_LSTM=p_LSTM, p_out=p_out\n",
        ")\n",
        "\n",
        "batch_size = 64\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "lr = 1e-2\n",
        "weight_decay = 1e-5\n",
        "optimizer = torch.optim.Adam(\n",
        "    model.parameters(),\n",
        "    lr=lr,\n",
        "    weight_decay=weight_decay,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "41fe0d3d",
      "metadata": {
        "id": "41fe0d3d"
      },
      "outputs": [],
      "source": [
        "# Split players into train/val/test\n",
        "draft_ids = drafts[\"draft_id\"].unique()\n",
        "\n",
        "train_ids, temp_ids = train_test_split(draft_ids, test_size=0.2, random_state=304)\n",
        "val_ids, test_ids = train_test_split(temp_ids, test_size=0.5, random_state=304)\n",
        "\n",
        "# Split Dataframe\n",
        "drafts_train = drafts[drafts[\"draft_id\"].isin(train_ids)]\n",
        "drafts_val = drafts[drafts[\"draft_id\"].isin(val_ids)]\n",
        "drafts_test = drafts[drafts[\"draft_id\"].isin(test_ids)]\n",
        "\n",
        "# Create custom Datasets for each split\n",
        "dataset_train = PlayerDataset(drafts_train)\n",
        "dataset_val = PlayerDataset(drafts_val)\n",
        "dataset_test = PlayerDataset(drafts_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "90d18fc0",
      "metadata": {
        "id": "90d18fc0"
      },
      "outputs": [],
      "source": [
        "# Create custom DataLoaders\n",
        "dlss = []\n",
        "for dataset in [dataset_train, dataset_val, dataset_test]:\n",
        "    dls = DataLoader(\n",
        "        dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_player_turns\n",
        "    )\n",
        "    dlss.append(dls)\n",
        "\n",
        "dls_train, dls_val, dls_test = dlss"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load deck evaluators\n",
        "%run \"deck_classification/deck_eval.ipynb\""
      ],
      "metadata": {
        "id": "tsCloz7mYxVL",
        "outputId": "717a69fa-7a21-4166-d09c-ce5a4ad37ef1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "tsCloz7mYxVL",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Config done before loading deck_eval.ipynb\n",
            "Collecting wget\n",
            "  Downloading wget-3.2.zip (10 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9655 sha256=e95e74291405ff53a7e0224091d48fe803c93078bb5dc44a8bef8f616dfb4ab6\n",
            "  Stored in directory: /root/.cache/pip/wheels/40/b3/0f/a40dbd1c6861731779f62cc4babcb234387e11d697df70ee97\n",
            "Successfully built wget\n",
            "Installing collected packages: wget\n",
            "Successfully installed wget-3.2\n",
            "Collecting spells-mtg\n",
            "  Downloading spells_mtg-0.11.10-py3-none-any.whl.metadata (47 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.4/47.4 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading spells_mtg-0.11.10-py3-none-any.whl (43 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.9/43.9 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: spells-mtg\n",
            "Successfully installed spells-mtg-0.11.10\n",
            "Config done before loading 17landsdataimport.ipynb\n",
            "Directory '/content/mtg-draft-agent/drive/MyDrive/Erdos25/MTGdraft/spells_data/DSK' copied successfully to '/root/.local/share/spells/external/DSK'.\n",
            "Remaining columns: []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def deck_list_to_vector(deck_list):\n",
        "  unique, counts = np.unique(deck_list, return_counts=True)\n",
        "  deck_dict = dict(zip(unique, counts))\n",
        "\n",
        "  deck_freq = np.zeros(vocab_size)\n",
        "  for i in range(vocab_size):\n",
        "      if i in deck_dict.keys():\n",
        "          deck_freq[i] = deck_dict[i]\n",
        "\n",
        "  return deck_freq"
      ],
      "metadata": {
        "id": "tdTvvHMEkQYD"
      },
      "id": "tdTvvHMEkQYD",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_epoch(\n",
        "    model, dataloader, optimizer, loss_fn, chunk_size=max_pack_size, device=None\n",
        "):\n",
        "    \"\"\"\n",
        "    chunk_size: Back propagate over a smaller number of turns.\n",
        "                The default is the size of a pack (i.e. the length of one \"round\" of\n",
        "                drafting).\n",
        "                If None, we backpropagate over all turns.\n",
        "    \"\"\"\n",
        "    # Move model to new device\n",
        "    if device is not None:\n",
        "        model = model.to(device)\n",
        "\n",
        "    # Initialize training mode\n",
        "    model.train()\n",
        "\n",
        "    # Remember: In PlayerDataset, each entry has the game information of a player,\n",
        "    # which consists of two lists of length equal to the number of turns\n",
        "    num_batches = len(dataloader)\n",
        "    num_players = len(dataloader.dataset)\n",
        "    num_turns = dataloader.dataset.num_turns()\n",
        "\n",
        "    # Accumulate the correct picks made by all players of each batch and on each turn\n",
        "    all_correct = torch.zeros(num_batches, num_turns)\n",
        "\n",
        "    # Deck evaluation store\n",
        "    deck_scores = torch.zeros(num_players)\n",
        "    print(deck_scores.shape)\n",
        "\n",
        "    # Accumulate loss over all players and all turns\n",
        "    total_loss = 0\n",
        "\n",
        "    # Each (pack_batches, pick_batches) is a list of turn states for a player batch\n",
        "    batch_count = 0\n",
        "    batch_idx = 0\n",
        "    for pack_batches, pick_batches in tqdm(dataloader):\n",
        "        # Each batch is a group of players, but the last batch may be smaller\n",
        "        batch_size = len(pack_batches[0])\n",
        "\n",
        "        # Store the final deck of every player\n",
        "        final_deck = torch.zeros((batch_size, num_turns))\n",
        "\n",
        "        # Initialize variables at the start of the game\n",
        "        batch_loss = 0\n",
        "        hidden_state = None\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # In case we want to backpropagate the whole game\n",
        "        if chunk_size is None:\n",
        "            chunk_size = num_turns\n",
        "\n",
        "        # Play game and backpropagate every chunk_size turns\n",
        "        for t0 in range(0, num_turns, chunk_size):\n",
        "            # End the chunk at the game's end, not later\n",
        "            chunk_end = min(t0 + chunk_size, num_turns)\n",
        "\n",
        "            # Play chunk_size turns\n",
        "            for t in range(t0, chunk_end):\n",
        "                pack_batch = pack_batches[t]\n",
        "                pick_batch = pick_batches[t]\n",
        "\n",
        "                if device is not None:\n",
        "                    pack_batch = pack_batch.to(device)\n",
        "                    pick_batch = pick_batch.to(device)\n",
        "\n",
        "                # Cards available to pick\n",
        "                pack_size = torch.tensor(pack_batch.shape[1], device=device)\n",
        "\n",
        "                # Forward pass -- remember hidden state from previous turn\n",
        "                logits, hidden_state = model(pack_batch, hidden_state=hidden_state)\n",
        "\n",
        "                # Note: logits is shaped (batch_size, seq_len, vocab_size) with seq_len=1\n",
        "                # but loss functions such as cross entropy expect shape\n",
        "                # (batch_size, vocab_size). That's why I slice here\n",
        "                logits = logits[:, -1, :]\n",
        "\n",
        "                # Accumulate losses of all players, normalized by pack size\n",
        "                # if pack_size > 1:\n",
        "                #     batch_loss += loss_fn(logits, pick_batch) / torch.log(pack_size)\n",
        "                # else:\n",
        "                #     batch_loss += loss_fn(logits, pick_batch)\n",
        "                batch_loss += loss_fn(logits, pick_batch)\n",
        "\n",
        "                # Count the number of players that picked the correct card\n",
        "                predictions = torch.argmax(logits, dim=-1)  # (batch,)\n",
        "                all_correct[batch_count, t] = (predictions == pick_batch).sum()\n",
        "\n",
        "                # Store decks\n",
        "                final_deck[:, t] = predictions\n",
        "\n",
        "            # I accumulated losses for several players across several turns.\n",
        "            # To keep magnitudes and variables interpretable (e.g. gradients),\n",
        "            # I backpropagate the average loss\n",
        "            played_turns = chunk_end - t0\n",
        "            mean_batch_loss = batch_loss / (batch_size)\n",
        "\n",
        "            # Backpropagate\n",
        "            mean_batch_loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Reset optimizer\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Detach hidden state to truncate gradients every chunk_size turns\n",
        "            hidden_state = tuple(h.detach() for h in hidden_state)\n",
        "\n",
        "            # Accumulate losses of all players\n",
        "            total_loss += batch_loss.item()\n",
        "\n",
        "        # Advance batch counter\n",
        "        batch_count += 1\n",
        "\n",
        "        # Evaluate decks\n",
        "        print(deck_scores.shape)\n",
        "        for idx in range(batch_size):\n",
        "          print(idx)\n",
        "          deck_list = final_deck[idx, :]\n",
        "          deck_freq = deck_list_to_vector(deck_list)\n",
        "          deck_freq = deck_freq[None, :]\n",
        "\n",
        "          # deck is a list of cards. Turn it into a vector\n",
        "          deck_scores[batch_idx + idx] = calculate_adjusted_win_rate_inverse_squared_weights(deck_freq)\n",
        "\n",
        "    # Add correct choices over all batches (i.e. over all players)\n",
        "    # then average over the number of players\n",
        "    # accuracy_per_turn = all_correct\n",
        "    accuracy_per_turn = all_correct.sum(dim=0) / num_players  # (num_turns,)\n",
        "\n",
        "    # Average total loss over the number of players and the number of turns\n",
        "    mean_loss = total_loss / (num_players)\n",
        "\n",
        "    return mean_loss, accuracy_per_turn, deck_scores\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(model, dataloader, loss_fn, device=None):\n",
        "    # Move model to new device\n",
        "    if device is not None:\n",
        "        model = model.to(device)\n",
        "\n",
        "    # Initialize evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    # Remember: In PlayerDataset, each entry has the game information of a player,\n",
        "    # which consists of two lists of length equal to the number of turns\n",
        "    num_batches = len(dataloader)\n",
        "    num_players = len(dataloader.dataset)\n",
        "    num_turns = dataloader.dataset.num_turns()\n",
        "\n",
        "    # Accumulate the correct picks made by all players of each batch and on each turn\n",
        "    all_correct = torch.zeros(num_batches, num_turns)\n",
        "\n",
        "    # Accumulate loss over all players and all turns\n",
        "    total_loss = 0\n",
        "\n",
        "    # Deck evaluation store\n",
        "    deck_scores = torch.zeros(num_players)\n",
        "\n",
        "    # Each (pack_batches, pick_batches) is a list of turn states for a player batch\n",
        "    batch_count = 0\n",
        "    batch_idx = 0\n",
        "    for pack_batches, pick_batches in dataloader:\n",
        "        # Each batch is a group of players, but the last batch may be smaller\n",
        "        batch_size = len(pack_batches[0])\n",
        "\n",
        "        # Store the final deck of every player\n",
        "        final_deck = torch.zeros((batch_size, num_turns))\n",
        "\n",
        "        # Initialize variables at the start of the game\n",
        "        batch_loss = 0\n",
        "        hidden_state = None\n",
        "\n",
        "        for t in range(num_turns):\n",
        "            # Extract turn info and move it to new device (if required)\n",
        "            pack_batch = pack_batches[t]\n",
        "            pick_batch = pick_batches[t]\n",
        "\n",
        "            if device is not None:\n",
        "                pack_batch = pack_batch.to(device)\n",
        "                pick_batch = pick_batch.to(device)\n",
        "\n",
        "            # Cards available to pick\n",
        "            pack_size = torch.tensor(pack_batch.shape[1], device=device)\n",
        "\n",
        "            # Forward pass\n",
        "            logits, hidden_state = model(pack_batch, hidden_state=hidden_state)\n",
        "\n",
        "            # Note: logits is shaped (batch_size, seq_len, vocab_size) with seq_len=1\n",
        "            # but loss functions such as cross entropy expect shape\n",
        "            # (batch_size, vocab_size). That's why I slice here\n",
        "            logits = logits[:, -1, :]\n",
        "\n",
        "            # Accumulate losses of all players, normalized by pack size\n",
        "            # if pack_size > 1:\n",
        "            #     batch_loss += loss_fn(logits, pick_batch) / torch.log(pack_size)\n",
        "            # else:\n",
        "            #     batch_loss += loss_fn(logits, pick_batch)\n",
        "            batch_loss += loss_fn(logits, pick_batch)\n",
        "\n",
        "            # Count the number of players that picked the correct card\n",
        "            predictions = torch.argmax(logits, dim=-1)  # (batch,)\n",
        "            all_correct[batch_count, t] = (predictions == pick_batch).sum()\n",
        "\n",
        "            # Store decks\n",
        "            final_deck[:, t] = predictions\n",
        "\n",
        "        # Accumulate batch losses. In total, this accumulates losses of all players\n",
        "        total_loss += batch_loss.item()\n",
        "\n",
        "        # Advance batch counter\n",
        "        batch_count += 1\n",
        "\n",
        "        # Evaluate decks\n",
        "        for idx in range(batch_size):\n",
        "            deck_list = final_deck[idx, :]\n",
        "            deck_freq = deck_list_to_vector(deck_list)\n",
        "\n",
        "            # deck is a list of cards. Turn it into a vector\n",
        "            deck_scores[batch_idx + idx] = calculate_adjusted_win_rate_inverse_squared_weights(deck_freq)\n",
        "\n",
        "    # Add correct choices over all batches (i.e. over all players)\n",
        "    # then average over the number of players\n",
        "    # accuracy_per_turn = all_correct\n",
        "    accuracy_per_turn = all_correct.sum(dim=0) / num_players  # (num_turns,)\n",
        "\n",
        "    # Average total loss over the number of players and the number of turns\n",
        "    mean_loss = total_loss / (num_players)\n",
        "\n",
        "    return mean_loss, accuracy_per_turn, deck_scores\n"
      ],
      "metadata": {
        "id": "cALZl751Yw0r"
      },
      "id": "cALZl751Yw0r",
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "b025dbaa",
      "metadata": {
        "id": "b025dbaa",
        "outputId": "f085b56b-3692-44f1-c627-9041c76058a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 717,
          "referenced_widgets": [
            "3284a1fe682741918a4bc38b90827d50",
            "c0c598da501e4a0e9e466bf1c71a1436",
            "365e43f5190f4c44a78057a380b901ee",
            "00b6cd31419144c28f7290041fb32e23",
            "55d1a1ff075b4f4ca620c2bda9322100",
            "7f12eff6301148569d46bb0550c58ad1",
            "9014da091c5d43d39406bc2afb93cf70",
            "27b480cb4d6a4225bd0a3e0dafcaaad6",
            "8e633461e2cc4ee2b8e3160520663a5f",
            "edc0bf8f0cc041e9a022827e87091d83",
            "a6b3697ea2074c13857aa2979dba9275"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([80])\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3284a1fe682741918a4bc38b90827d50"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([80])\n",
            "0\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 281 is different from 286)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4125816369.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;31m# Train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     train_loss, train_accuracy_epoch, train_deck_scores = train_epoch(\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdls_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunk_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     )\n",
            "\u001b[0;32m/tmp/ipython-input-3302701833.py\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(model, dataloader, optimizer, loss_fn, chunk_size, device)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m           \u001b[0;31m# deck is a list of cards. Turn it into a vector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m           \u001b[0mdeck_scores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_idx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_adjusted_win_rate_inverse_squared_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeck_freq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;31m# Add correct choices over all batches (i.e. over all players)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2579909991.py\u001b[0m in \u001b[0;36mcalculate_adjusted_win_rate_inverse_squared_weights\u001b[0;34m(data, cluster_data, n_closest_centers, columns)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcalculate_adjusted_win_rate_inverse_squared_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcluster_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcluster_data\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mn_closest_centers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdeck_atributes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0mclosest_5_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_5_closest_clusters_of_rows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcluster_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_closest_centers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_closest_centers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m   \u001b[0mwin_rates_all\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcluster_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'win_rate_adjusted'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mtotal_matches_all\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcluster_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'total_matches_adjusted'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2341380088.py\u001b[0m in \u001b[0;36mfind_5_closest_clusters_of_rows\u001b[0;34m(data, cluster_data, n_closest_centers, columns)\u001b[0m\n\u001b[1;32m     26\u001b[0m   \u001b[0;31m# sum(ab) is the dot product between each data point and each cluster centroid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m   \u001b[0;31m# This can be calculated efficiently using matrix multiplication: normalized_data @ normalized_clusters.T\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m   \u001b[0mdot_product\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormalized_data\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mnormalized_clusters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m   \u001b[0msquared_distance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mdot_product\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 281 is different from 286)"
          ]
        }
      ],
      "source": [
        "# Training loop parameters\n",
        "num_epochs = 25\n",
        "num_turns = dataset_train.num_turns()\n",
        "\n",
        "train_losses = torch.zeros(num_epochs)\n",
        "val_losses = torch.zeros(num_epochs)\n",
        "\n",
        "train_accuracies = torch.zeros((num_epochs, num_turns))\n",
        "val_accuracies = torch.zeros((num_epochs, num_turns))\n",
        "\n",
        "train_rating = torch.zeros(num_epochs)\n",
        "val_rating = torch.zeros(num_epochs)\n",
        "\n",
        "# Back propagate over the whole game\n",
        "# If we omit this line, we back propagate over a single round of the draft\n",
        "# (i.e. over 14 turns)\n",
        "chunk_size = None\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # Train\n",
        "    train_loss, train_accuracy_epoch, train_deck_scores = train_epoch(\n",
        "        model, dls_train, optimizer, loss_fn, chunk_size=None, device=device\n",
        "    )\n",
        "\n",
        "    # Evaluate on validation set\n",
        "    val_loss, val_accuracy_epoch, val_deck_scores = evaluate(model, dls_val, loss_fn, device=device)\n",
        "\n",
        "    # Print results\n",
        "    print(\n",
        "        f\"Epoch {epoch+1}/{num_epochs}: Train Loss = {train_loss:.4f}, Val Loss = {val_loss:.4f}\\n\"\n",
        "    )\n",
        "\n",
        "    train_losses[epoch] = train_loss\n",
        "    val_losses[epoch] = val_loss\n",
        "\n",
        "    train_accuracies[epoch, :] = train_accuracy_epoch.cpu()\n",
        "    val_accuracies[epoch, :] = val_accuracy_epoch.cpu()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "455775d4",
      "metadata": {
        "id": "455775d4"
      },
      "outputs": [],
      "source": [
        "# Calculate probability of random choice\n",
        "tt = np.arange(14, dtype=float)\n",
        "prob_random = 1 / np.flip(tt + 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3760ed14",
      "metadata": {
        "id": "3760ed14"
      },
      "outputs": [],
      "source": [
        "plt.plot(train_losses, label=\"Train\", marker=\".\")\n",
        "plt.plot(val_losses, label=\"Val\", marker=\".\")\n",
        "plt.legend()\n",
        "\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Training and Validation Losses\")\n",
        "\n",
        "y_max = plt.ylim()[1]\n",
        "y_max = utils.ceil_digit(y_max, digits=2)\n",
        "plt.ylim(0, y_max)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cfb8b7c0",
      "metadata": {
        "id": "cfb8b7c0"
      },
      "outputs": [],
      "source": [
        "# Separate packs\n",
        "plt.axvline(max_pack_size - 1, color=\"red\")\n",
        "plt.axvline(2 * max_pack_size - 1, color=\"red\")\n",
        "\n",
        "# Plot probability of random choice\n",
        "plt.plot(tt, prob_random, \"--\", color=\"blue\", label=\"Chance\")\n",
        "plt.plot(14 * 1 + tt, prob_random, \"--\", color=\"blue\")\n",
        "plt.plot(14 * 2 + tt, prob_random, \"--\", color=\"blue\")\n",
        "\n",
        "epoch = -1\n",
        "plt.plot(\n",
        "    train_accuracies[epoch, :], label=\"Train\", marker=\".\", markersize=15, color=\"orange\"\n",
        ")\n",
        "plt.plot(\n",
        "    val_accuracies[epoch, :], label=\"Val\", marker=\".\", markersize=10, color=\"green\"\n",
        ")\n",
        "plt.legend()\n",
        "\n",
        "plt.xlabel(\"Turn\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Training and Validation Accuracies per Turn\")\n",
        "\n",
        "plt.ylim([0, 1.05])\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8d433f0e",
      "metadata": {
        "id": "8d433f0e"
      },
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(), \"trained_models/lstm_params.pth\")"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "3284a1fe682741918a4bc38b90827d50": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c0c598da501e4a0e9e466bf1c71a1436",
              "IPY_MODEL_365e43f5190f4c44a78057a380b901ee",
              "IPY_MODEL_00b6cd31419144c28f7290041fb32e23"
            ],
            "layout": "IPY_MODEL_55d1a1ff075b4f4ca620c2bda9322100"
          }
        },
        "c0c598da501e4a0e9e466bf1c71a1436": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7f12eff6301148569d46bb0550c58ad1",
            "placeholder": "​",
            "style": "IPY_MODEL_9014da091c5d43d39406bc2afb93cf70",
            "value": "  0%"
          }
        },
        "365e43f5190f4c44a78057a380b901ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_27b480cb4d6a4225bd0a3e0dafcaaad6",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8e633461e2cc4ee2b8e3160520663a5f",
            "value": 0
          }
        },
        "00b6cd31419144c28f7290041fb32e23": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_edc0bf8f0cc041e9a022827e87091d83",
            "placeholder": "​",
            "style": "IPY_MODEL_a6b3697ea2074c13857aa2979dba9275",
            "value": " 0/2 [00:00&lt;?, ?it/s]"
          }
        },
        "55d1a1ff075b4f4ca620c2bda9322100": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7f12eff6301148569d46bb0550c58ad1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9014da091c5d43d39406bc2afb93cf70": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "27b480cb4d6a4225bd0a3e0dafcaaad6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8e633461e2cc4ee2b8e3160520663a5f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "edc0bf8f0cc041e9a022827e87091d83": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a6b3697ea2074c13857aa2979dba9275": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}