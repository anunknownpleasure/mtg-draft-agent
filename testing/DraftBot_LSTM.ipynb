{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "G1xYwD9XR700",
        "Hnnp1CnhWuep",
        "nxMXJBnUWGVZ"
      ],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOO/YPkv8zd9HFWJD2TG8/S",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/doctorsmylie/mtg-draft-agent/blob/main/testing/DraftBot_LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EV0wq_pb45cq"
      },
      "outputs": [],
      "source": [
        "# Configure Drive or Jupyter notebook -- only runs when first loaded\n",
        "if \"CONFIG_DONE\" not in globals():\n",
        "    # Need to mount drive and clone repo to access data and functions\n",
        "    try:\n",
        "        from google.colab import drive  # type: ignore\n",
        "\n",
        "        IN_COLAB = True\n",
        "\n",
        "        # clone repo\n",
        "        !git clone https://github.com/doctorsmylie/mtg-draft-agent\n",
        "        %cd mtg-draft-agent\n",
        "\n",
        "    except ModuleNotFoundError:\n",
        "        IN_COLAB = False\n",
        "\n",
        "    # Finish configuration -- also configures notebook outside of Colab\n",
        "    %run \"project_path.ipynb\"\n",
        "else:\n",
        "    print(\"Config done\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# from datasets import Dataset\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
        "\n",
        "import pathlib\n",
        "from itertools import product\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from time import time\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "import functions.card_io as card_io\n",
        "\n",
        "# Setting device on GPU if available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Using device:', device)"
      ],
      "metadata": {
        "id": "yO-XnR5mPj3v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_FOLDER_BU = DATA_FOLDER + '_backup'\n",
        "print(DATA_FOLDER_BU)"
      ],
      "metadata": {
        "id": "K1uJx-ZXqlmx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Expansion code\n",
        "expansion = \"DSK\"\n",
        "\n",
        "# Load draft history into a dataframe\n",
        "draftfilename = \"draft_data_public.\" + expansion + \".PremierDraft.csv.gz\"\n",
        "draft_file = pathlib.Path(DATA_FOLDER_BU, expansion, draftfilename)\n",
        "\n",
        "draftdata = pd.read_csv(draft_file, compression=\"gzip\", nrows=10000)"
      ],
      "metadata": {
        "id": "5nfeEe-5PqT6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_played_drafts_df(\n",
        "    draftdata,\n",
        "    card_to_idx=None,\n",
        "    num_packs=3,\n",
        "    num_picks=14,\n",
        "    prefix_pack=\"pack_card_\",\n",
        "    prefix_pool=\"pool_\",\n",
        "):\n",
        "  # Get unique ids\n",
        "  draft_ids = draftdata[\"draft_id\"].unique()\n",
        "\n",
        "  # Get columns with the player's options\n",
        "  pack_columns = draftdata.filter(regex=prefix_pack).columns\n",
        "  pack_columns = list(pack_columns)\n",
        "\n",
        "  # Get columns with the player's pool of cards\n",
        "  pool_columns = draftdata.filter(regex=prefix_pool).columns\n",
        "  pool_columns = list(pool_columns)\n",
        "\n",
        "  # Get the card-index dictionary if we don't already have it\n",
        "  if card_to_idx is None:\n",
        "    card_names, card_to_idx, idx_to_card = card_io.get_cards_from_draft_df(draftdata)\n",
        "\n",
        "  # Compile data for each draft_id\n",
        "  draft_list = []\n",
        "\n",
        "  for i, id in enumerate(draft_ids):\n",
        "    time_start = time()\n",
        "\n",
        "    # Get draft info for id\n",
        "    data_id = draftdata.loc[draftdata[\"draft_id\"] == id, :]\n",
        "\n",
        "    # Check that we have the right amount of data\n",
        "    num_rows = data_id.shape[0]\n",
        "    if num_rows != num_packs * num_picks:\n",
        "      print(f\"{i+1}/{len(draft_ids)}\", end=\": \")\n",
        "      print(\n",
        "          f\"Draft incomplete. Only {num_rows} out of {num_packs*num_picks} rows. Skipping id {id}.\"\n",
        "      )\n",
        "      draft_ids = np.delete(draft_ids, i)\n",
        "\n",
        "      continue\n",
        "\n",
        "    # Build iterators to extract information in turn order\n",
        "    draft_turns = product(range(num_packs), range(num_picks))\n",
        "\n",
        "    for pack_idx, pick_idx in draft_turns:\n",
        "      # Get row for the turn by filtering pack number, pick number, and draft id\n",
        "      df_turn = draftdata[\n",
        "          (draftdata[\"draft_id\"] == id)\n",
        "          & (draftdata[\"pack_number\"] == pack_idx)\n",
        "          & (draftdata[\"pick_number\"] == pick_idx)\n",
        "      ]\n",
        "\n",
        "      # Get pick, cards in pack, and cards in pool\n",
        "      df_index = df_turn.index[0]\n",
        "      pick = df_turn.at[df_index, \"pick\"]\n",
        "      cards_in_pack = card_io.count_to_list(df_turn, prefix_pack)\n",
        "      cards_in_pool = card_io.count_to_list(df_turn, prefix_pool)\n",
        "\n",
        "      # Store results\n",
        "      chosen = card_to_idx[pick]\n",
        "      pack = [card_to_idx[card] for card in cards_in_pack]\n",
        "      pool = [card_to_idx[card] for card in cards_in_pool]\n",
        "\n",
        "      row = pd.Series({\n",
        "          'draft_id': id,\n",
        "          'pack_number': pack_idx,\n",
        "          'pick_number': pick_idx,\n",
        "          'pick': chosen,\n",
        "          'pack': pack,\n",
        "          'pool': pool\n",
        "      })\n",
        "      draft_list.append(row)\n",
        "\n",
        "    time_end = time()\n",
        "    dt = time_end - time_start\n",
        "    print(f\"{i+1}/{len(draft_ids)}: {np.round(dt,3)}\")\n",
        "  print()\n",
        "\n",
        "  # Convert draft_list into DataFrame\n",
        "  time_start = time()\n",
        "  drafts = pd.DataFrame(draft_list,\n",
        "                        columns=['draft_id', 'pack_number', 'pick_number', 'pick', 'pack', 'pool'])\n",
        "  time_end = time()\n",
        "  print(f\"End: {np.round(dt,3)}\")\n",
        "\n",
        "  return drafts\n"
      ],
      "metadata": {
        "id": "magOFYtQcGIm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get unique draft ids\n",
        "draft_ids = draftdata[\"draft_id\"].unique()\n",
        "\n",
        "# Get card names and card-index dictionaries\n",
        "card_names, card_to_idx, idx_to_card = card_io.get_cards_from_draft_df(draftdata)\n",
        "vocab_size = len(card_names)\n",
        "\n",
        "# Get draft history\n",
        "drafts = get_played_drafts_df(draftdata, card_to_idx)\n",
        "max_pack_size = drafts['pick_number'].max()+1\n",
        "\n",
        "# Use expansion size as padding_idx\n",
        "PAD_IDX = vocab_size"
      ],
      "metadata": {
        "id": "uhBQBY8hQdHZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drafts.head(5)"
      ],
      "metadata": {
        "id": "wZ4piVLUbxPs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LSTM\n",
        "Use an LSTM to build a bot.\n",
        "\n",
        "Unlike a Transformer, this architecture can be trained sequentially out of the box. We could force a Transformer to train sequentially, but then we'd lose the parallelism advantage and use more parameters than an LSTM. At least now we have a smaller model."
      ],
      "metadata": {
        "id": "twRCMSWO5C2f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tensorize_lists(*args, padding_value=PAD_IDX, device=None):\n",
        "  \"\"\"\n",
        "  Turns the list of arguments into a padded tensor.\n",
        "  - Each argument is a list of tokens, and they will be padded to have the same\n",
        "    length.\n",
        "  - Returns the padded tensor and the original length of each argument\n",
        "  \"\"\"\n",
        "  # Change arguments to tensors if they aren't already\n",
        "  pack_tensors = []\n",
        "  for pack in args:\n",
        "    # Turn list into a tensor\n",
        "    if not isinstance(pack, torch.Tensor):\n",
        "      # If device doesn't matter\n",
        "      if device is None:\n",
        "        pack_tensors.append(torch.tensor(pack))\n",
        "      # Create on specified device\n",
        "      else:\n",
        "        pack_tensors.append(torch.tensor(pack, device=device))\n",
        "    # Store the tensors\n",
        "    else:\n",
        "      # Device doesn't matter\n",
        "      if device is None:\n",
        "        pack_tensors.append(pack)\n",
        "      # Move to specified device\n",
        "      else:\n",
        "        pack_tensors.append(pack.to(device))\n",
        "\n",
        "  # Store lengths\n",
        "  lengths = [pack.shape[0] for pack in pack_tensors]\n",
        "  pack_size = max(lengths)\n",
        "\n",
        "  # Pad to shape (batch_size, pack_size)\n",
        "  pack_batch = pad_sequence(pack_tensors, batch_first=True, padding_value=padding_value)\n",
        "\n",
        "  return pack_batch, lengths"
      ],
      "metadata": {
        "id": "9Z8MkHP-FM1N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DraftBotLSTM(nn.Module):\n",
        "  def __init__(self, vocab_size, embed_dim, hidden_dim, p_LSTM=0.0, p_out=0.0):\n",
        "    super().__init__()\n",
        "\n",
        "    # Attributes\n",
        "    self.vocab_size = vocab_size\n",
        "\n",
        "    # For padding sequences. We later add a \"word\" to the embedding layer to\n",
        "    # encode this new padding index\n",
        "    self.padding_idx = PAD_IDX\n",
        "\n",
        "    # LSTM followed by a full layer that produces logits\n",
        "    self.embedding = nn.Embedding(vocab_size+1, embed_dim, padding_idx=self.padding_idx)\n",
        "    self.lstm = nn.LSTM(\n",
        "        embed_dim,\n",
        "        hidden_dim,\n",
        "        batch_first=True,\n",
        "        dropout=p_LSTM)\n",
        "    self.dropout = nn.Dropout(p=p_out)\n",
        "    self.output_layer = nn.Linear(hidden_dim, vocab_size)  # maps hidden state to logits\n",
        "\n",
        "  # Returns the device the model sits on\n",
        "  def device(self):\n",
        "    return next(self.parameters()).device\n",
        "\n",
        "  def forward(self, pack_batch, hidden_state=None):\n",
        "    \"\"\"\n",
        "    pack_batch: [Tensor] of shape (batch_size, pack_size). Contains a batch of packs.\n",
        "    hidden_state: Tuple of (h_0, c_0), each of shape (batch_size, 1, hidden_dim)\n",
        "    Returns:\n",
        "        logits: shape (batch_size, seq_len, pack_size) with seq_len=1\n",
        "        hidden_state: final hidden state (for next step)\n",
        "    \"\"\"\n",
        "    pack_embed = self.embedding(pack_batch) # (batch_size, pack_size, embed_dim)\n",
        "    # print(' ------------ ')\n",
        "    # print('Embedding:', pack_embed.shape)\n",
        "\n",
        "    # Careful with padded tokens\n",
        "    mask = (pack_batch != self.padding_idx).unsqueeze(-1)  # (batch_size, seq_len, 1)\n",
        "    valid_counts = mask.sum(dim=1, keepdim=True)  # (batch_size, 1, 1)\n",
        "\n",
        "    # pool cards into a single vector (i.e. change pack_size to 1)\n",
        "    pack_pooled = pack_embed.sum(dim=1, keepdim=True)  # shape (batch_size, 1, embed_dim)\n",
        "    pack_pooled = pack_pooled/valid_counts\n",
        "\n",
        "    # print('Embedding pooled:', pack_pooled.shape)\n",
        "    # Q: Do I need to average the embeddings? Isn't this lossy?\n",
        "\n",
        "    # Pass through LSTM\n",
        "    lstm_out, hidden = self.lstm(pack_pooled, hidden_state)  # shape: (batch_size, 1, hidden_dim)\n",
        "\n",
        "    # print()\n",
        "    # print('LSTM output')\n",
        "    # print('lstm_out:', lstm_out.shape)\n",
        "    # print('hidden state:', len(hidden))\n",
        "    # for i,h_i in enumerate(hidden):\n",
        "#       print(f'hidden_{i}: {h_i.shape}')\n",
        "    # print()\n",
        "\n",
        "    # Dropout layer\n",
        "    lstm_out_drop = self.dropout(lstm_out)\n",
        "\n",
        "    # Map LSTM output to logits\n",
        "    logits = self.output_layer(lstm_out_drop)  # shape: (batch_size, 1, vocab_size)\n",
        "    # print('Logits:', logits.shape)\n",
        "    # print()\n",
        "\n",
        "    return logits, hidden\n",
        "\n",
        "  def predict(self, pack_batch, hidden_state=None):\n",
        "    \"\"\"\n",
        "    pack_batch: [Tensor] of shape (batch_size, pack_size). Contains a batch of packs.\n",
        "    hidden_state: Tuple of (h_0, c_0), each of shape (batch_size, 1, hidden_dim)\n",
        "    Returns:\n",
        "        predictions: shape (batch_size,) The chosen card token from each batch\n",
        "        hidden_state: final hidden state\n",
        "    \"\"\"\n",
        "    # pack.shape = (batch_size, pack_size)\n",
        "    logits, hidden = self(pack_batch, hidden_state)  # shape: (batch_size, 1, vocab_size)\n",
        "\n",
        "    # Get predictions from the last time step\n",
        "    last_logits = logits[:, -1, :]  # (batch_size, vocab_size)\n",
        "    predictions = torch.argmax(last_logits, dim=-1)  # (batch,)\n",
        "\n",
        "    return predictions, hidden\n",
        "\n",
        "  def predict_single(self, pack, hidden_state=None):\n",
        "    \"\"\"\n",
        "    pack: List of card tokens. We turn the pack into a tensor of shape (batch_size, pack_size)\n",
        "          with batch_size=1.\n",
        "    hidden_state: Tuple of (h_0, c_0), each of shape (batch_size, 1, hidden_dim)\n",
        "    Returns:\n",
        "        prediction: The chosen card token from the pack.\n",
        "        hidden_state: final hidden state\n",
        "    \"\"\"\n",
        "    pack_batch = tensorize_lists(pack, device=self.device())\n",
        "    return self.predict(pack_batch, hidden_state)\n",
        "\n",
        "  def predict_batch(self, pack_list, hidden_state=None):\n",
        "    \"\"\"\n",
        "    pack_list: List of card packs. Each pack is a list of card tokens.\n",
        "              We turn the pack list into a tensor of shape (batch_size, pack_size)\n",
        "              with batch_size=len(pack_list), and we pad all packs to the maximum\n",
        "              pack size.\n",
        "    hidden_state: Tuple of (h_0, c_0), each of shape (batch_size, 1, hidden_dim)\n",
        "    Returns:\n",
        "        predictions: The chosen card token from each pack.\n",
        "        hidden_state: final hidden state\n",
        "    \"\"\"\n",
        "    pack_batch, lengths = tensorize_lists(*pack_list,\n",
        "                                          device=self.device(),\n",
        "                                          padding_value=self.padding_idx)\n",
        "    # pack_padded = pack_padded_sequence(pack_batch, lengths, batch_first=True, enforce_sorted=False)\n",
        "\n",
        "    # Obtain predictions\n",
        "    predictions, hidden = self.predict(pack_batch, hidden_state)\n",
        "    # Note: No need to pad_packed_sequence because I return a single token\n",
        "\n",
        "    return predictions, hidden"
      ],
      "metadata": {
        "id": "L-VCB_be47PT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initial testing"
      ],
      "metadata": {
        "id": "G1xYwD9XR700"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# vocab_size, embed_dim, hidden_dim\n",
        "DraftBot0 = DraftBotLSTM(100, 250, 150)\n",
        "\n",
        "card_packs = [\n",
        "    [1,3,5,7,9],\n",
        "    [11,13,15,17],\n",
        "    [21,23,25,27,29,31]\n",
        "]\n",
        "# card_packs = [torch.tensor(pack) for pack in card_packs]\n",
        "\n",
        "for pack in card_packs:\n",
        "  print(len(pack))"
      ],
      "metadata": {
        "id": "TTKygtWDBVZs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predictions by hand\n",
        "logit_list = []\n",
        "choices = []\n",
        "hidden_state = None\n",
        "for pack in card_packs:\n",
        "  input = torch.tensor(pack).unsqueeze(0)\n",
        "  # input = torch.tensor([pack]*2)\n",
        "\n",
        "  # print('Input:', input.shape)\n",
        "  # print('Raw:', pack)\n",
        "  # print('Tensor:', input)\n",
        "  # print()\n",
        "\n",
        "  logits, hidden_state = DraftBot0.forward(input, hidden_state)\n",
        "  logit_list.append(logits)\n",
        "  choices.append(torch.argmax(logits, dim=-1).squeeze())\n",
        "\n",
        "  print('logits:', logits.shape)\n",
        "  print('choice:', choices[-1].shape)\n",
        "  print()\n",
        "\n",
        "print('Choices:', choices)"
      ],
      "metadata": {
        "id": "dJBXoFqwBmMP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Using .predict_single\n",
        "choices = []\n",
        "hidden_state = None\n",
        "for pack in card_packs:\n",
        "  prediction, hidden = DraftBot0.predict_single(pack)\n",
        "  choices.append(prediction)\n",
        "\n",
        "print('Choices:', choices)"
      ],
      "metadata": {
        "id": "GK9zVAWYFxu2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Using .predict_batch\n",
        "# Using .predict_single\n",
        "choices, hidden = DraftBot0.predict_batch(card_packs)\n",
        "print('Choices:', choices)"
      ],
      "metadata": {
        "id": "htYRYU9RiptL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mock dataset"
      ],
      "metadata": {
        "id": "Hnnp1CnhWuep"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Base model\n",
        "class TokenSelectionLSTM(nn.Module):\n",
        "  def __init__(self, vocab_size, embed_dim, hidden_dim):\n",
        "    super().__init__()\n",
        "    self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "    self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n",
        "    self.output_layer = nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "  def forward(self, x, hidden=None):\n",
        "    # x shape: (batch_size, seq_len)\n",
        "    x = self.embedding(x)  # -> (batch_size, seq_len, embed_dim)\n",
        "    output, hidden = self.lstm(x, hidden)  # output: (batch_size, seq_len, hidden_dim)\n",
        "    logits = self.output_layer(output)  # -> (batch_size, seq_len, vocab_size)\n",
        "    return logits, hidden\n",
        "\n",
        "  def predict(self, x, hidden_state=None):\n",
        "    logits, hidden = self(x, hidden_state)\n",
        "\n",
        "    # Get predictions from the last time step\n",
        "    last_logits = logits[:, -1, :]  # (batch, vocab_size)\n",
        "    predictions = torch.argmax(last_logits, dim=-1)  # (batch,)\n",
        "    return predictions, hidden\n"
      ],
      "metadata": {
        "id": "tRZATrMVX9ng"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mock dataset\n",
        "vocab_size = 50\n",
        "seq_len = 5\n",
        "batch_size = 32\n",
        "n_samples = 1000\n",
        "\n",
        "torch.manual_seed(0)\n",
        "X = torch.randint(0, vocab_size, (n_samples, seq_len))  # (n_samples, seq_len)\n",
        "y = torch.randint(0, vocab_size, (n_samples, 1))  # targets per position\n",
        "\n",
        "dataset = torch.utils.data.TensorDataset(X, y)\n",
        "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)"
      ],
      "metadata": {
        "id": "o5iR_Gb1WxKJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model, Loss and Optimizer\n",
        "embed_dim = 64\n",
        "hidden_dim = 128\n",
        "model = DraftBotLSTM(vocab_size, embed_dim, hidden_dim)\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
      ],
      "metadata": {
        "id": "Qoi8t89aW1tr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "epochs = 5\n",
        "for epoch in range(epochs):\n",
        "    total_loss = 0\n",
        "\n",
        "    for xb, yb in dataloader:\n",
        "        logits, _ = model(xb)  # (batch, seq_len, vocab)\n",
        "        # Flatten for CrossEntropyLoss\n",
        "        loss = loss_fn(logits.view(-1, vocab_size), yb.view(-1))\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item() * xb.size(0)\n",
        "\n",
        "    avg_loss = total_loss / len(dataset)\n",
        "    print(f\"Epoch {epoch+1}: loss={avg_loss:.4f}\")"
      ],
      "metadata": {
        "id": "JUo2pgQvW_GK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show some predictions to see how well the model trained\n",
        "for idx, (xb, yb) in enumerate(dataloader):\n",
        "  prediction, hidden = model.predict(xb)\n",
        "\n",
        "  results = torch.stack([yb.T, prediction.unsqueeze(0)])\n",
        "\n",
        "  print('True/Pred')\n",
        "  print(results)\n",
        "  print()\n",
        "\n",
        "  if idx>5:\n",
        "    break"
      ],
      "metadata": {
        "id": "ZpQ8I5FQnhYj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Technical checks"
      ],
      "metadata": {
        "id": "-hDo8iAunf9d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(xb.shape)\n",
        "yb_pred, _ = model(xb)\n",
        "\n",
        "print(yb_pred.shape)"
      ],
      "metadata": {
        "id": "KEJGp2drYIQM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count = 0\n",
        "for xb, yb in dataloader:\n",
        "  print(count)\n",
        "  print(xb.shape)\n",
        "  print(yb.shape)\n",
        "  print()\n",
        "\n",
        "  count += 1"
      ],
      "metadata": {
        "id": "EszxtoopYXqh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Try to imitate human drafting"
      ],
      "metadata": {
        "id": "ZmX0qvJ2R9jG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Custom Dataset and DataLoaders"
      ],
      "metadata": {
        "id": "nxMXJBnUWGVZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset to return a single player's game\n",
        "class PlayerDataset(Dataset):\n",
        "  \"\"\"\n",
        "  A Dataset class that handles the players in our dataset. When called, it\n",
        "  returns all the turns of a chosen player.\n",
        "  \"\"\"\n",
        "  def __init__(self, df):\n",
        "    self.df = df\n",
        "    self.players = df['draft_id'].unique()\n",
        "    self.padding_idx = PAD_IDX\n",
        "\n",
        "  def __len__(self):\n",
        "    \"\"\"\n",
        "    Number of players in our dataset\n",
        "    \"\"\"\n",
        "    return len(self.players)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    \"\"\"\n",
        "    Returns the lists of packs and picks of the player with index idx.\n",
        "    Packs and picks are sorted by turn order.\n",
        "    \"\"\"\n",
        "    id = self.players[idx]\n",
        "    df_player = self.df[self.df['draft_id'] == id]\n",
        "\n",
        "    # Make sure that turns are sorted correctly\n",
        "    df_player = df_player.sort_values(by=['pack_number', 'pick_number'], ascending=True)\n",
        "\n",
        "    # Extract packs and picks in turn sequence\n",
        "    packs = []\n",
        "    picks = []\n",
        "    for idx, row in df_player.iterrows():\n",
        "      # LSTMs (and tokens in general) work best with long dtype\n",
        "      pack = torch.tensor(row['pack'], dtype=torch.long)\n",
        "      pick = torch.tensor(row['pick'], dtype=torch.long)\n",
        "\n",
        "      packs.append(pack)\n",
        "      picks.append(pick)\n",
        "\n",
        "    # Return the lists of packs and picks of a single player\n",
        "    return packs, picks\n",
        "\n",
        "  def __repr__(self):\n",
        "    return f\"PlayerDataset with {len(self)} players\\n{self.df.to_string(max_rows=5)}\"\n",
        "\n",
        "# Custom collate function.\n",
        "# It's used for training the LSTM batching by player\n",
        "def collate_player_turns(batch):\n",
        "  \"\"\"\n",
        "  Collate function for PlayerDataset. Creates batches of players, and returns\n",
        "  a list of batches. Each batch contains the game state (packs and picks) in a\n",
        "  single turn, and for all the player in the batch. The list is sorted\n",
        "  chronologically by turn.\n",
        "  We assume all players have the same number of turns, and the same number of\n",
        "  options in each turn.\n",
        "\n",
        "  args:\n",
        "  batch: list of tuples (packs, picks). Each tuple contains the draft data\n",
        "  of a single player in turn order.\n",
        "\n",
        "  Returns:\n",
        "  pack_batches: list of packs for each turn. Each element is a tensor of shape\n",
        "                (batch_size, pack_size), where batch_size is the number of players.\n",
        "  pick_batches: list of picks for each turn. Each element is a tensor of shape\n",
        "                (batch_size,), where batch_size is the number of players.\n",
        "  \"\"\"\n",
        "  # batch: list of tuples (packs, picks). length = number of players\n",
        "\n",
        "  # batch[0] = (packs, picks) of first player\n",
        "  # num. turns = len(packs), equiv. len(picks)\n",
        "  n_turns = len(batch[0][0])\n",
        "\n",
        "  # pack and pick batches, sorted by turn\n",
        "  # Element idx will be the play info of all players in turn idx\n",
        "  pack_batches = []\n",
        "  pick_batches = []\n",
        "  for turn in range(n_turns):\n",
        "    # Extract turn info from all players\n",
        "    packs_turn = []\n",
        "    picks_turn = []\n",
        "    for player_pack, player_pick in batch:\n",
        "      packs_turn.append(player_pack[turn])\n",
        "      picks_turn.append(player_pick[turn])\n",
        "\n",
        "    # Stack and store\n",
        "    # We're assuming all players have the same number of options in each turn,\n",
        "    # so we can stack their packs without padding.\n",
        "    pack_batches.append(torch.stack(packs_turn))\n",
        "    pick_batches.append(torch.stack(picks_turn))\n",
        "\n",
        "  return pack_batches, pick_batches"
      ],
      "metadata": {
        "id": "ilee-35_uhS5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training and evaluation functions"
      ],
      "metadata": {
        "id": "EJqe3NMQWNkw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training and evaluation functions\n",
        "def train_epoch(model, dataloader, optimizer, loss_fn, chunk_size=max_pack_size, device=None):\n",
        "  \"\"\"\n",
        "  chunk_size: Back propagate over a smaller number of turns.\n",
        "              The default is the size of a pack (i.e. the length of one \"round\" of\n",
        "              drafting).\n",
        "              If None, we backpropagate over all turns.\n",
        "  \"\"\"\n",
        "  # Move model to new device\n",
        "  if device is not None:\n",
        "    model = model.to(device)\n",
        "\n",
        "  # Initialize training mode\n",
        "  model.train()\n",
        "\n",
        "  # Remember: In PlayerDataset, each player's entry has two lists, and both\n",
        "  # their lengths are equal to the number of turns.\n",
        "  num_turns = len(dataloader.dataset[0][0])\n",
        "\n",
        "  # Each batch is a group of players, but the last batch may be smaller\n",
        "  num_players = len(dataloader.dataset)\n",
        "  num_batches = len(dataloader)\n",
        "\n",
        "  # For each batch, track accuracy in all turns\n",
        "  accuracy_per_batch = torch.zeros(num_batches, num_turns)\n",
        "\n",
        "  # Each (pack_batches, pick_batches) is a list of turn states for a player batch\n",
        "  total_loss = 0\n",
        "  batch_count = 0\n",
        "  for pack_batches, pick_batches in tqdm(dataloader):\n",
        "    batch_size = len(pack_batches[0])\n",
        "\n",
        "    # Initialize variables at the start of the game\n",
        "    mean_batch_loss = 0\n",
        "    hidden_state = None\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # In case we want to backpropagate the whole game\n",
        "    if chunk_size is None:\n",
        "      chunk_size = num_turns\n",
        "\n",
        "    # Play game and backpropagate periodically\n",
        "    for t0 in range(0, num_turns, chunk_size):\n",
        "      # End the chunk at the game's end, not later\n",
        "      chunk_end = min(t0+chunk_size, num_turns)\n",
        "\n",
        "      # Play chunk_size turns\n",
        "      for t in range(t0, chunk_end):\n",
        "        pack_batch = pack_batches[t]\n",
        "        pick_batch = pick_batches[t]\n",
        "\n",
        "        if device is not None:\n",
        "          pack_batch = pack_batch.to(device)\n",
        "          pick_batch = pick_batch.to(device)\n",
        "\n",
        "        # Forward pass -- remember hidden state from previous turn\n",
        "        logits, hidden_state = model(pack_batch, hidden_state=hidden_state)\n",
        "\n",
        "        # Note: logits is shaped (batch_size, seq_len, vocab_size) with seq_len=1\n",
        "        # but loss functions such as cross entropy expect shape\n",
        "        # (batch_size, vocab_size). That's why I slice here\n",
        "        logits = logits[:, -1, :]\n",
        "\n",
        "        # Accumulate loss averaged over batches\n",
        "        mean_batch_loss += loss_fn(logits, pick_batch)/batch_size\n",
        "\n",
        "        # Count the number of players that picked the correct card\n",
        "        predictions = torch.argmax(logits, dim=-1)  # (batch,)\n",
        "        accuracy_per_batch[batch_count, t] = (predictions == pick_batch).sum()\n",
        "\n",
        "      # Backpropagate over chunk_size turns\n",
        "      mean_batch_loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      # Reset optimizer\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      # Detach hidden state to truncate gradients\n",
        "      hidden_state = tuple(h.detach() for h in hidden_state)\n",
        "\n",
        "      # Accumulate total loss (need to unnnormalize batch_loss)\n",
        "      total_loss += mean_batch_loss.item() * batch_size\n",
        "      batch_count += 1\n",
        "\n",
        "  # Find total number of correct predictions, then compute accuracy\n",
        "  accuracy_per_turn = accuracy_per_batch.sum(dim=0) / num_players  #(num_turns,)\n",
        "\n",
        "  # Average total loss over the number of players\n",
        "  loss_average = total_loss / num_players\n",
        "\n",
        "  return loss_average, accuracy_per_turn\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(model, dataloader, loss_fn, device=None):\n",
        "  # Move model to new device\n",
        "  if device is not None:\n",
        "    model = model.to(device)\n",
        "\n",
        "  # Initialize evaluation mode\n",
        "  model.eval()\n",
        "  total_loss = 0\n",
        "\n",
        "  # Each (pack_batches, pick_batches) is a list of turn states for a player batch\n",
        "  total_loss = 0\n",
        "  for pack_batches, pick_batches in dataloader:\n",
        "    num_turns = len(pack_batches)\n",
        "    batch_size = len(pack_batches[0])\n",
        "\n",
        "    # Initialize variables at the start of the game\n",
        "    batch_loss = 0\n",
        "    hidden_state = None\n",
        "\n",
        "    for t in range(num_turns):\n",
        "      # Extract turn info and move it to new device (if required)\n",
        "      pack_batch = pack_batches[t]\n",
        "      pick_batch = pick_batches[t]\n",
        "\n",
        "      if device is not None:\n",
        "        pack_batch = pack_batch.to(device)\n",
        "        pick_batch = pick_batch.to(device)\n",
        "\n",
        "      # Forward pass\n",
        "      logits, hidden_state = model(pack_batch, hidden_state=hidden_state)\n",
        "\n",
        "      # Accumulate losses of all players\n",
        "        # Note: logits is shaped (batch_size, seq_len, vocab_size) with seq_len=1\n",
        "        # but loss functions such as cross entropy expect shape\n",
        "        # (batch_size, vocab_size). That's why I slice here\n",
        "      batch_loss += loss_fn(logits[:,-1,:], pick_batch)\n",
        "\n",
        "    # Accumulate batch losses. In total, this accumulates losses of all players\n",
        "    # in the dataloader\n",
        "    total_loss += batch_loss.item()\n",
        "\n",
        "  # Return the average player loss\n",
        "  return total_loss / len(dataloader.dataset)"
      ],
      "metadata": {
        "id": "g_4bMepdUg4I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train the model"
      ],
      "metadata": {
        "id": "AYyeyG61WP9S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split players into train/val/test\n",
        "draft_ids = drafts['draft_id'].unique()\n",
        "\n",
        "train_ids, temp_ids = train_test_split(draft_ids, test_size=0.2, random_state=304)\n",
        "val_ids, test_ids = train_test_split(temp_ids, test_size=0.5, random_state=304)\n",
        "\n",
        "# Split Dataframe\n",
        "drafts_train = drafts[drafts['draft_id'].isin(train_ids)]\n",
        "drafts_val = drafts[drafts['draft_id'].isin(val_ids)]\n",
        "drafts_test = drafts[drafts['draft_id'].isin(test_ids)]\n",
        "\n",
        "# Create custom Datasets for each split\n",
        "dataset_train = PlayerDataset(drafts_train)\n",
        "dataset_val = PlayerDataset(drafts_val)\n",
        "dataset_test = PlayerDataset(drafts_test)"
      ],
      "metadata": {
        "id": "UgrGCH0jGyyB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model, Loss and Optimizer\n",
        "embed_dim = 32\n",
        "hidden_dim = 128\n",
        "p_LSTM = 0\n",
        "p_out = 0.5\n",
        "model = DraftBotLSTM(\n",
        "    vocab_size,\n",
        "    embed_dim,\n",
        "    hidden_dim,\n",
        "    p_LSTM=p_LSTM,\n",
        "    p_out=p_out\n",
        ")\n",
        "\n",
        "batch_size = 32\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "lr = 5e-3\n",
        "weight_decay = 1e-5\n",
        "optimizer = torch.optim.Adam(\n",
        "    model.parameters(),\n",
        "    lr=1e-3,\n",
        "    weight_decay=weight_decay,\n",
        ")"
      ],
      "metadata": {
        "id": "Oh5HMaBZEByw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create custom DataLoaders\n",
        "dlss = []\n",
        "for dataset in [dataset_train, dataset_val, dataset_test]:\n",
        "  dls = DataLoader(\n",
        "      dataset,\n",
        "      batch_size=batch_size,\n",
        "      shuffle=False,\n",
        "      collate_fn=collate_player_turns\n",
        "      )\n",
        "  dlss.append(dls)\n",
        "\n",
        "dls_train, dls_val, dls_test = dlss"
      ],
      "metadata": {
        "id": "vfRmSyWEMO2b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop parameters\n",
        "num_epochs = 25\n",
        "\n",
        "train_losses = np.zeros(num_epochs)\n",
        "val_losses = np.zeros(num_epochs)\n",
        "\n",
        "# Back propagate over the whole game\n",
        "# If we omit this line, we back propagate over a single round of the draft\n",
        "# (i.e. over 14 turns)\n",
        "chunck_size = None\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  # Train\n",
        "  train_loss = train_epoch(\n",
        "      model,\n",
        "      dls_train,\n",
        "      optimizer,\n",
        "      loss_fn,\n",
        "      chunk_size=None,\n",
        "      device=device\n",
        "      )\n",
        "\n",
        "  # Evaluate on validation set\n",
        "  val_loss = evaluate(\n",
        "      model,\n",
        "      dls_val,\n",
        "      loss_fn,\n",
        "      device=device\n",
        "      )\n",
        "\n",
        "  # Print results\n",
        "  print(f\"Epoch {epoch+1}/{num_epochs}: Train Loss = {train_loss:.4f}, Val Loss = {val_loss:.4f}\\n\")\n",
        "  train_losses[epoch] = train_loss\n",
        "  val_losses[epoch] = val_loss\n"
      ],
      "metadata": {
        "id": "4rhlWHu4NNPI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(train_losses, label='Train', marker='.')\n",
        "plt.plot(val_losses, label='Val', marker='.')\n",
        "plt.legend()\n",
        "\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and Validation Losses')\n",
        "\n",
        "y_max = plt.ylim()[1]\n",
        "y_max = np.ceil(y_max)\n",
        "plt.ylim(0, y_max)"
      ],
      "metadata": {
        "id": "hm7gWFkyGamg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QA9InNWXNeeo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}