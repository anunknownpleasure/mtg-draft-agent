{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/doctorsmylie/mtg-draft-agent/blob/main/testing/DraftBot_LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EV0wq_pb45cq"
   },
   "outputs": [],
   "source": [
    "# Configure Drive or Jupyter notebook -- only runs when first loaded\n",
    "if \"CONFIG_DONE\" not in globals():\n",
    "    # Need to mount drive and clone repo to access data and functions\n",
    "    try:\n",
    "        from google.colab import drive  # type: ignore\n",
    "\n",
    "        IN_COLAB = True\n",
    "\n",
    "        # clone repo\n",
    "        !git clone https://github.com/doctorsmylie/mtg-draft-agent\n",
    "        %cd mtg-draft-agent\n",
    "\n",
    "    except ModuleNotFoundError:\n",
    "        IN_COLAB = False\n",
    "\n",
    "    # Finish configuration -- also configures notebook outside of Colab\n",
    "    %run \"project_path.ipynb\"\n",
    "else:\n",
    "    print(\"Config done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yO-XnR5mPj3v"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# from datasets import Dataset\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "import pathlib\n",
    "from itertools import product\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from time import time\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import functions.card_io as card_io\n",
    "\n",
    "# Setting device on GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K1uJx-ZXqlmx"
   },
   "outputs": [],
   "source": [
    "DATA_FOLDER_BU = DATA_FOLDER + \"_backup\"\n",
    "print(DATA_FOLDER_BU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5nfeEe-5PqT6"
   },
   "outputs": [],
   "source": [
    "# Expansion code\n",
    "expansion = \"DSK\"\n",
    "\n",
    "# Load draft history into a dataframe\n",
    "draftfilename = \"draft_data_public.\" + expansion + \".PremierDraft.csv.gz\"\n",
    "draft_file = pathlib.Path(DATA_FOLDER, expansion, draftfilename)\n",
    "\n",
    "draftdata = pd.read_csv(draft_file, compression=\"gzip\", nrows=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uhBQBY8hQdHZ"
   },
   "outputs": [],
   "source": [
    "# Get unique draft ids\n",
    "draft_ids = draftdata[\"draft_id\"].unique()\n",
    "\n",
    "# Get card names and card-index dictionaries\n",
    "card_names, card_to_idx, idx_to_card = card_io.get_cards_from_draft_df(draftdata)\n",
    "vocab_size = len(card_names)\n",
    "\n",
    "# Get draft history\n",
    "drafts = card_io.get_played_drafts(draftdata, card_to_idx)\n",
    "max_pack_size = drafts[\"pick_number\"].max() + 1\n",
    "\n",
    "# Use expansion size as padding_idx\n",
    "PAD_IDX = vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wZ4piVLUbxPs"
   },
   "outputs": [],
   "source": [
    "drafts.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "twRCMSWO5C2f"
   },
   "source": [
    "# LSTM\n",
    "Use an LSTM to build a bot.\n",
    "\n",
    "Unlike a Transformer, this architecture can be trained sequentially out of the box. We could force a Transformer to train sequentially, but then we'd lose the parallelism advantage and use more parameters than an LSTM. At least now we have a smaller model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9Z8MkHP-FM1N"
   },
   "outputs": [],
   "source": [
    "def tensorize_lists(*args, padding_value=PAD_IDX, device=None):\n",
    "    \"\"\"\n",
    "    Turns the list of arguments into a padded tensor.\n",
    "    - Each argument is a list of tokens, and they will be padded to have the same\n",
    "      length.\n",
    "    - Returns the padded tensor and the original length of each argument\n",
    "    \"\"\"\n",
    "    # Change arguments to tensors if they aren't already\n",
    "    pack_tensors = []\n",
    "    for pack in args:\n",
    "        # Turn list into a tensor\n",
    "        if not isinstance(pack, torch.Tensor):\n",
    "            # If device doesn't matter\n",
    "            if device is None:\n",
    "                pack_tensors.append(torch.tensor(pack))\n",
    "            # Create on specified device\n",
    "            else:\n",
    "                pack_tensors.append(torch.tensor(pack, device=device))\n",
    "        # Store the tensors\n",
    "        else:\n",
    "            # Device doesn't matter\n",
    "            if device is None:\n",
    "                pack_tensors.append(pack)\n",
    "            # Move to specified device\n",
    "            else:\n",
    "                pack_tensors.append(pack.to(device))\n",
    "\n",
    "    # Store lengths\n",
    "    lengths = [pack.shape[0] for pack in pack_tensors]\n",
    "    pack_size = max(lengths)\n",
    "\n",
    "    # Pad to shape (batch_size, pack_size)\n",
    "    pack_batch = pad_sequence(\n",
    "        pack_tensors, batch_first=True, padding_value=padding_value\n",
    "    )\n",
    "\n",
    "    return pack_batch, lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L-VCB_be47PT"
   },
   "outputs": [],
   "source": [
    "class DraftBotLSTM(nn.Module):\n",
    "    def __init__(\n",
    "        self, vocab_size, embed_dim, hidden_dim, padding_idx=None, p_LSTM=0.0, p_out=0.0\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # Attributes\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "        # For padding sequences. We later add a \"word\" to the embedding layer to\n",
    "        # encode this new padding index\n",
    "        if padding_idx is None:\n",
    "            self.padding_idx = vocab_size\n",
    "        else:\n",
    "            self.padding_idx = padding_idx\n",
    "\n",
    "        # LSTM followed by a full layer that produces logits\n",
    "        self.embedding = nn.Embedding(\n",
    "            vocab_size + 1, embed_dim, padding_idx=self.padding_idx\n",
    "        )\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True, dropout=p_LSTM)\n",
    "        self.dropout = nn.Dropout(p=p_out)\n",
    "        self.output_layer = nn.Linear(\n",
    "            hidden_dim, vocab_size\n",
    "        )  # maps hidden state to logits\n",
    "\n",
    "    # Returns the device the model sits on\n",
    "    def device(self):\n",
    "        return next(self.parameters()).device\n",
    "\n",
    "    def forward(self, pack_batch, hidden_state=None):\n",
    "        \"\"\"\n",
    "        pack_batch: [Tensor] of shape (batch_size, pack_size). Contains a batch of packs.\n",
    "        hidden_state: Tuple of (h_0, c_0), each of shape (batch_size, 1, hidden_dim)\n",
    "        Returns:\n",
    "            logits: shape (batch_size, seq_len, pack_size) with seq_len=1\n",
    "            hidden_state: final hidden state (for next step)\n",
    "        \"\"\"\n",
    "        pack_embed = self.embedding(pack_batch)  # (batch_size, pack_size, embed_dim)\n",
    "        # print(' ------------ ')\n",
    "        # print('Embedding:', pack_embed.shape)\n",
    "\n",
    "        # Careful with padded tokens\n",
    "        mask = (pack_batch != self.padding_idx).unsqueeze(\n",
    "            -1\n",
    "        )  # (batch_size, seq_len, 1)\n",
    "        valid_counts = mask.sum(dim=1, keepdim=True)  # (batch_size, 1, 1)\n",
    "\n",
    "        # pool cards into a single vector (i.e. change pack_size to 1)\n",
    "        pack_pooled = pack_embed.sum(\n",
    "            dim=1, keepdim=True\n",
    "        )  # shape (batch_size, 1, embed_dim)\n",
    "        pack_pooled = pack_pooled / valid_counts\n",
    "\n",
    "        # print('Embedding pooled:', pack_pooled.shape)\n",
    "        # Q: Do I need to average the embeddings? Isn't this lossy?\n",
    "\n",
    "        # Pass through LSTM\n",
    "        lstm_out, hidden = self.lstm(\n",
    "            pack_pooled, hidden_state\n",
    "        )  # shape: (batch_size, 1, hidden_dim)\n",
    "\n",
    "        # print()\n",
    "        # print('LSTM output')\n",
    "        # print('lstm_out:', lstm_out.shape)\n",
    "        # print('hidden state:', len(hidden))\n",
    "        # for i,h_i in enumerate(hidden):\n",
    "        #       print(f'hidden_{i}: {h_i.shape}')\n",
    "        # print()\n",
    "\n",
    "        # Dropout layer\n",
    "        lstm_out_drop = self.dropout(lstm_out)\n",
    "\n",
    "        # Map LSTM output to logits\n",
    "        logits = self.output_layer(lstm_out_drop)  # shape: (batch_size, 1, vocab_size)\n",
    "        # print('Logits:', logits.shape)\n",
    "        # print()\n",
    "\n",
    "        return logits, hidden\n",
    "\n",
    "    def predict(self, pack_batch, hidden_state=None):\n",
    "        \"\"\"\n",
    "        pack_batch: [Tensor] of shape (batch_size, pack_size). Contains a batch of packs.\n",
    "        hidden_state: Tuple of (h_0, c_0), each of shape (batch_size, 1, hidden_dim)\n",
    "        Returns:\n",
    "            predictions: shape (batch_size,) The chosen card token from each batch\n",
    "            hidden_state: final hidden state\n",
    "        \"\"\"\n",
    "        # pack.shape = (batch_size, pack_size)\n",
    "        logits, hidden = self(\n",
    "            pack_batch, hidden_state\n",
    "        )  # shape: (batch_size, 1, vocab_size)\n",
    "\n",
    "        # Get predictions from the last time step\n",
    "        last_logits = logits[:, -1, :]  # (batch_size, vocab_size)\n",
    "        predictions = torch.argmax(last_logits, dim=-1)  # (batch,)\n",
    "\n",
    "        return predictions, hidden\n",
    "\n",
    "    def predict_single(self, pack, hidden_state=None):\n",
    "        \"\"\"\n",
    "        pack: List of card tokens. We turn the pack into a tensor of shape (batch_size, pack_size)\n",
    "              with batch_size=1.\n",
    "        hidden_state: Tuple of (h_0, c_0), each of shape (batch_size, 1, hidden_dim)\n",
    "        Returns:\n",
    "            prediction: The chosen card token from the pack.\n",
    "            hidden_state: final hidden state\n",
    "        \"\"\"\n",
    "        pack_batch, _ = tensorize_lists(\n",
    "            pack, device=self.device(), padding_value=self.padding_idx\n",
    "        )\n",
    "\n",
    "        return self.predict(pack_batch, hidden_state)\n",
    "\n",
    "    def predict_batch(self, pack_list, hidden_state=None):\n",
    "        \"\"\"\n",
    "        pack_list: List of card packs. Each pack is a list of card tokens.\n",
    "                  We turn the pack list into a tensor of shape (batch_size, pack_size)\n",
    "                  with batch_size=len(pack_list), and we pad all packs to the maximum\n",
    "                  pack size.\n",
    "        hidden_state: Tuple of (h_0, c_0), each of shape (batch_size, 1, hidden_dim)\n",
    "        Returns:\n",
    "            predictions: The chosen card token from each pack.\n",
    "            hidden_state: final hidden state\n",
    "        \"\"\"\n",
    "        pack_batch, _ = tensorize_lists(\n",
    "            *pack_list, device=self.device(), padding_value=self.padding_idx\n",
    "        )\n",
    "        # pack_padded = pack_padded_sequence(pack_batch, lengths, batch_first=True, enforce_sorted=False)\n",
    "\n",
    "        # Obtain predictions\n",
    "        predictions, hidden = self.predict(pack_batch, hidden_state)\n",
    "        # Note: No need to pad_packed_sequence because I return a single token\n",
    "\n",
    "        return predictions, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G1xYwD9XR700"
   },
   "source": [
    "## Initial testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TTKygtWDBVZs"
   },
   "outputs": [],
   "source": [
    "# vocab_size, embed_dim, hidden_dim\n",
    "vocab_size = 100\n",
    "embed_dim = 250\n",
    "hidden_dim = 150\n",
    "DraftBot0 = DraftBotLSTM(vocab_size, embed_dim, hidden_dim)\n",
    "\n",
    "card_packs = [[1, 3, 5, 7, 9], [11, 13, 15, 17], [21, 23, 25, 27, 29, 31]]\n",
    "# card_packs = [torch.tensor(pack) for pack in card_packs]\n",
    "\n",
    "for pack in card_packs:\n",
    "    print(len(pack))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dJBXoFqwBmMP"
   },
   "outputs": [],
   "source": [
    "# Predictions by hand\n",
    "logit_list = []\n",
    "choices = []\n",
    "hidden_state = None\n",
    "for pack in card_packs:\n",
    "    input = torch.tensor(pack).unsqueeze(0)\n",
    "    # input = torch.tensor([pack]*2)\n",
    "\n",
    "    # print('Input:', input.shape)\n",
    "    # print('Raw:', pack)\n",
    "    # print('Tensor:', input)\n",
    "    # print()\n",
    "\n",
    "    logits, hidden_state = DraftBot0.forward(input, hidden_state)\n",
    "    logit_list.append(logits)\n",
    "    choices.append(torch.argmax(logits, dim=-1).squeeze())\n",
    "\n",
    "    print(\"logits:\", logits.shape)\n",
    "    print(\"choice:\", choices[-1].shape)\n",
    "    print()\n",
    "\n",
    "print(\"Choices:\", choices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GK9zVAWYFxu2"
   },
   "outputs": [],
   "source": [
    "# Using .predict_single\n",
    "choices = []\n",
    "hidden_state = None\n",
    "for pack in card_packs:\n",
    "    prediction, hidden = DraftBot0.predict_single(pack)\n",
    "    choices.append(prediction)\n",
    "\n",
    "print(\"Choices:\", choices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "htYRYU9RiptL"
   },
   "outputs": [],
   "source": [
    "# Using .predict_batch\n",
    "# Using .predict_single\n",
    "choices, hidden = DraftBot0.predict_batch(card_packs)\n",
    "print(\"Choices:\", choices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hnnp1CnhWuep"
   },
   "source": [
    "# Mock dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o5iR_Gb1WxKJ"
   },
   "outputs": [],
   "source": [
    "vocab_size = 50\n",
    "seq_len = 5\n",
    "batch_size = 32\n",
    "n_samples = 1000\n",
    "\n",
    "torch.manual_seed(0)\n",
    "X = torch.randint(0, vocab_size, (n_samples, seq_len))  # (n_samples, seq_len)\n",
    "y = torch.randint(0, vocab_size, (n_samples, 1))  # targets per position\n",
    "\n",
    "dataset = torch.utils.data.TensorDataset(X, y)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qoi8t89aW1tr"
   },
   "outputs": [],
   "source": [
    "# Model, Loss and Optimizer\n",
    "embed_dim = 64\n",
    "hidden_dim = 128\n",
    "model = DraftBotLSTM(vocab_size, embed_dim, hidden_dim)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JUo2pgQvW_GK"
   },
   "outputs": [],
   "source": [
    "# Training loop\n",
    "epochs = 5\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "\n",
    "    for xb, yb in dataloader:\n",
    "        logits, _ = model(xb)  # (batch, seq_len, vocab)\n",
    "        # Flatten for CrossEntropyLoss\n",
    "        loss = loss_fn(logits.view(-1, vocab_size), yb.view(-1))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * xb.size(0)\n",
    "\n",
    "    avg_loss = total_loss / len(dataset)\n",
    "    print(f\"Epoch {epoch+1}: loss={avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZpQ8I5FQnhYj"
   },
   "outputs": [],
   "source": [
    "# Show some predictions to see how well the model trained\n",
    "for idx, (xb, yb) in enumerate(dataloader):\n",
    "    prediction, hidden = model.predict(xb)\n",
    "\n",
    "    results = torch.stack([yb.T, prediction.unsqueeze(0)])\n",
    "\n",
    "    print(\"True/Pred\")\n",
    "    print(results)\n",
    "    print()\n",
    "\n",
    "    if idx > 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-hDo8iAunf9d"
   },
   "source": [
    "### Technical checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KEJGp2drYIQM"
   },
   "outputs": [],
   "source": [
    "print(xb.shape)\n",
    "yb_pred, _ = model(xb)\n",
    "\n",
    "print(yb_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EszxtoopYXqh"
   },
   "outputs": [],
   "source": [
    "count = 0\n",
    "for xb, yb in dataloader:\n",
    "    print(count)\n",
    "    print(xb.shape)\n",
    "    print(yb.shape)\n",
    "    print()\n",
    "\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZmX0qvJ2R9jG"
   },
   "source": [
    "# Try to imitate human drafting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nxMXJBnUWGVZ"
   },
   "source": [
    "## Custom Dataset and DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ilee-35_uhS5"
   },
   "outputs": [],
   "source": [
    "# Dataset to return a single player's game\n",
    "class PlayerDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A Dataset class that handles the players in our dataset. When called, it\n",
    "    returns all the turns of a chosen player.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        self.players = df[\"draft_id\"].unique()\n",
    "        self.padding_idx = PAD_IDX\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Number of players in our dataset\n",
    "        \"\"\"\n",
    "        return len(self.players)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Returns the lists of packs and picks of the player with index idx.\n",
    "        Packs and picks are sorted by turn order.\n",
    "        \"\"\"\n",
    "        id = self.players[idx]\n",
    "        df_player = self.df[self.df[\"draft_id\"] == id]\n",
    "\n",
    "        # Make sure that turns are sorted correctly\n",
    "        df_player = df_player.sort_values(\n",
    "            by=[\"pack_number\", \"pick_number\"], ascending=True\n",
    "        )\n",
    "\n",
    "        # Extract packs and picks in turn sequence\n",
    "        packs = []\n",
    "        picks = []\n",
    "        for idx, row in df_player.iterrows():\n",
    "            # LSTMs (and tokens in general) work best with long dtype\n",
    "            pack = torch.tensor(row[\"pack\"], dtype=torch.long)\n",
    "            pick = torch.tensor(row[\"pick\"], dtype=torch.long)\n",
    "\n",
    "            packs.append(pack)\n",
    "            picks.append(pick)\n",
    "\n",
    "        # Return the lists of packs and picks of a single player\n",
    "        return packs, picks\n",
    "\n",
    "    def __repr__(self):\n",
    "        return (\n",
    "            f\"PlayerDataset with {len(self)} players\\n{self.df.to_string(max_rows=5)}\"\n",
    "        )\n",
    "\n",
    "\n",
    "# Custom collate function.\n",
    "# It's used for training the LSTM batching by player\n",
    "def collate_player_turns(batch):\n",
    "    \"\"\"\n",
    "    Collate function for PlayerDataset. Creates batches of players, and returns\n",
    "    a list of batches. Each batch contains the game state (packs and picks) in a\n",
    "    single turn, and for all the player in the batch. The list is sorted\n",
    "    chronologically by turn.\n",
    "    We assume all players have the same number of turns, and the same number of\n",
    "    options in each turn.\n",
    "\n",
    "    args:\n",
    "    batch: list of tuples (packs, picks). Each tuple contains the draft data\n",
    "    of a single player in turn order.\n",
    "\n",
    "    Returns:\n",
    "    pack_batches: list of packs for each turn. Each element is a tensor of shape\n",
    "                  (batch_size, pack_size), where batch_size is the number of players.\n",
    "    pick_batches: list of picks for each turn. Each element is a tensor of shape\n",
    "                  (batch_size,), where batch_size is the number of players.\n",
    "    \"\"\"\n",
    "    # batch: list of tuples (packs, picks). length = number of players\n",
    "\n",
    "    # batch[0] = (packs, picks) of first player\n",
    "    # num. turns = len(packs), equiv. len(picks)\n",
    "    n_turns = len(batch[0][0])\n",
    "\n",
    "    # pack and pick batches, sorted by turn\n",
    "    # Element idx will be the play info of all players in turn idx\n",
    "    pack_batches = []\n",
    "    pick_batches = []\n",
    "    for turn in range(n_turns):\n",
    "        # Extract turn info from all players\n",
    "        packs_turn = []\n",
    "        picks_turn = []\n",
    "        for player_pack, player_pick in batch:\n",
    "            packs_turn.append(player_pack[turn])\n",
    "            picks_turn.append(player_pick[turn])\n",
    "\n",
    "        # Stack and store\n",
    "        # We're assuming all players have the same number of options in each turn,\n",
    "        # so we can stack their packs without padding.\n",
    "        pack_batches.append(torch.stack(packs_turn))\n",
    "        pick_batches.append(torch.stack(picks_turn))\n",
    "\n",
    "    return pack_batches, pick_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EJqe3NMQWNkw"
   },
   "source": [
    "## Training and evaluation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g_4bMepdUg4I"
   },
   "outputs": [],
   "source": [
    "# Training and evaluation functions\n",
    "def train_epoch(\n",
    "    model, dataloader, optimizer, loss_fn, chunk_size=max_pack_size, device=None\n",
    "):\n",
    "    \"\"\"\n",
    "    chunk_size: Back propagate over a smaller number of turns.\n",
    "                The default is the size of a pack (i.e. the length of one \"round\" of\n",
    "                drafting).\n",
    "                If None, we backpropagate over all turns.\n",
    "    \"\"\"\n",
    "    # Move model to new device\n",
    "    if device is not None:\n",
    "        model = model.to(device)\n",
    "\n",
    "    # Initialize training mode\n",
    "    model.train()\n",
    "\n",
    "    # Remember: In PlayerDataset, each player's entry has two lists, and both\n",
    "    # their lengths are equal to the number of turns.\n",
    "    num_turns = len(dataloader.dataset[0][0])\n",
    "\n",
    "    # Each batch is a group of players, but the last batch may be smaller\n",
    "    num_players = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "\n",
    "    # For each batch, track accuracy in all turns\n",
    "    accuracy_per_batch = torch.zeros(num_batches, num_turns)\n",
    "\n",
    "    # Each (pack_batches, pick_batches) is a list of turn states for a player batch\n",
    "    total_loss = 0\n",
    "    batch_count = 0\n",
    "    for pack_batches, pick_batches in tqdm(dataloader):\n",
    "        batch_size = len(pack_batches[0])\n",
    "\n",
    "        # Initialize variables at the start of the game\n",
    "        mean_batch_loss = 0\n",
    "        hidden_state = None\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # In case we want to backpropagate the whole game\n",
    "        if chunk_size is None:\n",
    "            chunk_size = num_turns\n",
    "\n",
    "        # Play game and backpropagate periodically\n",
    "        for t0 in range(0, num_turns, chunk_size):\n",
    "            # End the chunk at the game's end, not later\n",
    "            chunk_end = min(t0 + chunk_size, num_turns)\n",
    "\n",
    "            # Play chunk_size turns\n",
    "            for t in range(t0, chunk_end):\n",
    "                pack_batch = pack_batches[t]\n",
    "                pick_batch = pick_batches[t]\n",
    "\n",
    "                if device is not None:\n",
    "                    pack_batch = pack_batch.to(device)\n",
    "                    pick_batch = pick_batch.to(device)\n",
    "\n",
    "                # Forward pass -- remember hidden state from previous turn\n",
    "                logits, hidden_state = model(pack_batch, hidden_state=hidden_state)\n",
    "\n",
    "                # Note: logits is shaped (batch_size, seq_len, vocab_size) with seq_len=1\n",
    "                # but loss functions such as cross entropy expect shape\n",
    "                # (batch_size, vocab_size). That's why I slice here\n",
    "                logits = logits[:, -1, :]\n",
    "\n",
    "                # Accumulate loss averaged over batches\n",
    "                mean_batch_loss += loss_fn(logits, pick_batch) / batch_size\n",
    "\n",
    "                # Count the number of players that picked the correct card\n",
    "                predictions = torch.argmax(logits, dim=-1)  # (batch,)\n",
    "                accuracy_per_batch[batch_count, t] = (predictions == pick_batch).sum()\n",
    "\n",
    "            # Backpropagate over chunk_size turns\n",
    "            mean_batch_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Reset optimizer\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Detach hidden state to truncate gradients\n",
    "            hidden_state = tuple(h.detach() for h in hidden_state)\n",
    "\n",
    "            # Accumulate total loss (need to unnnormalize batch_loss)\n",
    "            total_loss += mean_batch_loss.item() * batch_size\n",
    "            batch_count += 1\n",
    "\n",
    "    # Find total number of correct predictions, then compute accuracy\n",
    "    accuracy_per_turn = accuracy_per_batch.sum(dim=0) / num_players  # (num_turns,)\n",
    "\n",
    "    # Average total loss over the number of players\n",
    "    loss_average = total_loss / num_players\n",
    "\n",
    "    return loss_average, accuracy_per_turn\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, dataloader, loss_fn, device=None):\n",
    "    # Move model to new device\n",
    "    if device is not None:\n",
    "        model = model.to(device)\n",
    "\n",
    "    # Initialize evaluation mode\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "\n",
    "    # Each (pack_batches, pick_batches) is a list of turn states for a player batch\n",
    "    total_loss = 0\n",
    "    for pack_batches, pick_batches in dataloader:\n",
    "        num_turns = len(pack_batches)\n",
    "        batch_size = len(pack_batches[0])\n",
    "\n",
    "        # Initialize variables at the start of the game\n",
    "        batch_loss = 0\n",
    "        hidden_state = None\n",
    "\n",
    "        for t in range(num_turns):\n",
    "            # Extract turn info and move it to new device (if required)\n",
    "            pack_batch = pack_batches[t]\n",
    "            pick_batch = pick_batches[t]\n",
    "\n",
    "            if device is not None:\n",
    "                pack_batch = pack_batch.to(device)\n",
    "                pick_batch = pick_batch.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            logits, hidden_state = model(pack_batch, hidden_state=hidden_state)\n",
    "\n",
    "            # Accumulate losses of all players\n",
    "            # Note: logits is shaped (batch_size, seq_len, vocab_size) with seq_len=1\n",
    "            # but loss functions such as cross entropy expect shape\n",
    "            # (batch_size, vocab_size). That's why I slice here\n",
    "            batch_loss += loss_fn(logits[:, -1, :], pick_batch)\n",
    "\n",
    "        # Accumulate batch losses. In total, this accumulates losses of all players\n",
    "        # in the dataloader\n",
    "        total_loss += batch_loss.item()\n",
    "\n",
    "    # Return the average player loss\n",
    "    return total_loss / len(dataloader.dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AYyeyG61WP9S"
   },
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UgrGCH0jGyyB"
   },
   "outputs": [],
   "source": [
    "# Split players into train/val/test\n",
    "draft_ids = drafts[\"draft_id\"].unique()\n",
    "\n",
    "train_ids, temp_ids = train_test_split(draft_ids, test_size=0.2, random_state=304)\n",
    "val_ids, test_ids = train_test_split(temp_ids, test_size=0.5, random_state=304)\n",
    "\n",
    "# Split Dataframe\n",
    "drafts_train = drafts[drafts[\"draft_id\"].isin(train_ids)]\n",
    "drafts_val = drafts[drafts[\"draft_id\"].isin(val_ids)]\n",
    "drafts_test = drafts[drafts[\"draft_id\"].isin(test_ids)]\n",
    "\n",
    "# Create custom Datasets for each split\n",
    "dataset_train = PlayerDataset(drafts_train)\n",
    "dataset_val = PlayerDataset(drafts_val)\n",
    "dataset_test = PlayerDataset(drafts_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Oh5HMaBZEByw"
   },
   "outputs": [],
   "source": [
    "# Model, Loss and Optimizer\n",
    "embed_dim = 32\n",
    "hidden_dim = 128\n",
    "p_LSTM = 0\n",
    "p_out = 0.5\n",
    "model = DraftBotLSTM(vocab_size, embed_dim, hidden_dim, p_LSTM=p_LSTM, p_out=p_out)\n",
    "\n",
    "batch_size = 32\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "lr = 5e-3\n",
    "weight_decay = 1e-5\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=1e-3,\n",
    "    weight_decay=weight_decay,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vfRmSyWEMO2b"
   },
   "outputs": [],
   "source": [
    "# Create custom DataLoaders\n",
    "dlss = []\n",
    "for dataset in [dataset_train, dataset_val, dataset_test]:\n",
    "    dls = DataLoader(\n",
    "        dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_player_turns\n",
    "    )\n",
    "    dlss.append(dls)\n",
    "\n",
    "dls_train, dls_val, dls_test = dlss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4rhlWHu4NNPI"
   },
   "outputs": [],
   "source": [
    "# Training loop parameters\n",
    "num_epochs = 25\n",
    "\n",
    "train_losses = np.zeros(num_epochs)\n",
    "val_losses = np.zeros(num_epochs)\n",
    "\n",
    "# Back propagate over the whole game\n",
    "# If we omit this line, we back propagate over a single round of the draft\n",
    "# (i.e. over 14 turns)\n",
    "chunck_size = None\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Train\n",
    "    train_loss = train_epoch(\n",
    "        model, dls_train, optimizer, loss_fn, chunk_size=None, device=device\n",
    "    )\n",
    "\n",
    "    # Evaluate on validation set\n",
    "    val_loss = evaluate(model, dls_val, loss_fn, device=device)\n",
    "\n",
    "    # Print results\n",
    "    print(\n",
    "        f\"Epoch {epoch+1}/{num_epochs}: Train Loss = {train_loss:.4f}, Val Loss = {val_loss:.4f}\\n\"\n",
    "    )\n",
    "    train_losses[epoch] = train_loss\n",
    "    val_losses[epoch] = val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hm7gWFkyGamg"
   },
   "outputs": [],
   "source": [
    "plt.plot(train_losses, label=\"Train\", marker=\".\")\n",
    "plt.plot(val_losses, label=\"Val\", marker=\".\")\n",
    "plt.legend()\n",
    "\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training and Validation Losses\")\n",
    "\n",
    "y_max = plt.ylim()[1]\n",
    "y_max = np.ceil(y_max)\n",
    "plt.ylim(0, y_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QA9InNWXNeeo"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOO/YPkv8zd9HFWJD2TG8/S",
   "collapsed_sections": [
    "G1xYwD9XR700",
    "Hnnp1CnhWuep",
    "nxMXJBnUWGVZ"
   ],
   "gpuType": "T4",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "erdos25",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
