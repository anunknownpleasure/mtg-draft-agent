{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNYN2p6kT7Rm1qYSGUYh76u",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/doctorsmylie/mtg-draft-agent/blob/main/deck_classification/deck_eval.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "INIT CONFIG"
      ],
      "metadata": {
        "id": "Y5s8yRzxWkcB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Configure Drive or Jupyter notebook -- only runs when first loaded\n",
        "if \"CONFIG_DONE\" not in globals():\n",
        "    # Need to mount drive and clone repo to access data and functions\n",
        "    try:\n",
        "        from google.colab import drive  # type: ignore\n",
        "\n",
        "        IN_COLAB = True\n",
        "\n",
        "        # clone repo\n",
        "        !git clone https://github.com/doctorsmylie/mtg-draft-agent\n",
        "        %cd mtg-draft-agent\n",
        "\n",
        "    except ModuleNotFoundError:\n",
        "        IN_COLAB = False\n",
        "\n",
        "    # Finish configuration -- also configures notebook outside of Colab\n",
        "    %run \"project_path.ipynb\"\n",
        "else:\n",
        "    print(\"Config done before loading deck_eval.ipynb\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iiKQN0tFthjV",
        "outputId": "883cc17a-e0f2-4c4b-d188-1481085d1ff7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'mtg-draft-agent'...\n",
            "remote: Enumerating objects: 232, done.\u001b[K\n",
            "remote: Counting objects: 100% (14/14), done.\u001b[K\n",
            "remote: Compressing objects: 100% (10/10), done.\u001b[K\n",
            "remote: Total 232 (delta 2), reused 10 (delta 2), pack-reused 218 (from 1)\u001b[K\n",
            "Receiving objects: 100% (232/232), 11.97 MiB | 27.17 MiB/s, done.\n",
            "Resolving deltas: 100% (122/122), done.\n",
            "/content/mtg-draft-agent\n",
            "Starting config...\n",
            "Running in Colab? Yes\n",
            "\n",
            "Configuring Google Colab...\n",
            "Mounting Drive...\n",
            "Mounted at /content/mtg-draft-agent/drive\n",
            "BASE_PATH =  /content/mtg-draft-agent\n",
            "DATA_FOLDER = /content/mtg-draft-agent/drive/MyDrive/Erdos25/MTGdraft\n",
            "BASE_PATH == os.getcwd(): True\n",
            "\n",
            "Configuration done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Load deck data and column name lists"
      ],
      "metadata": {
        "id": "LGtFJjzIWCFd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%run \"17landsdataimport.ipynb\""
      ],
      "metadata": {
        "id": "bpyb1tEFVIHD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load Cluster Data"
      ],
      "metadata": {
        "id": "PQBL_FpVXkZU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_cluster_data(file_path):\n",
        "  \"\"\"\n",
        "  Loads the cluster data from a parquet file into a polars DataFrame.\n",
        "\n",
        "  Args:\n",
        "    file_path: The path to the parquet file.\n",
        "\n",
        "  Returns:\n",
        "    A polars DataFrame containing the cluster data.\n",
        "  \"\"\"\n",
        "  cluster_data = pl.read_parquet(file_path)\n",
        "  return cluster_data"
      ],
      "metadata": {
        "id": "L_dMeSkJXj1w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cluster_data=load_cluster_data(DATA_PATH + '/cluster_data.parquet')"
      ],
      "metadata": {
        "id": "UH5V1tmaX3GD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Load Scaler and PCA"
      ],
      "metadata": {
        "id": "OyK6oHbcV7It"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "def load_scaler(file_path):\n",
        "    with open(file_path, 'rb') as f:\n",
        "        scaler = pickle.load(f)\n",
        "    return scaler\n",
        "\n",
        "def load_pca_model(file_path):\n",
        "    with open(file_path, 'rb') as f:\n",
        "        pca_model = pickle.load(f)\n",
        "    return pca_model"
      ],
      "metadata": {
        "id": "uKUInSnKVIf6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = load_scaler(DATA_PATH + '/scaler.pkl')\n",
        "pca_model = load_pca_model(DATA_PATH + '/pca_model.pkl')"
      ],
      "metadata": {
        "id": "ZNEAHRR2YN0h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Scale/PCA/Normalization transform"
      ],
      "metadata": {
        "id": "tvgsDrlWV1NH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_data(data):\n",
        "  if not isinstance(data, np.ndarray):\n",
        "    try:\n",
        "      data = np.array(data)\n",
        "    except:\n",
        "      raise Exception('data must be arraylike')\n",
        "\n",
        "  return data/(np.linalg.norm(data, axis=1).reshape(-1,1))\n",
        "\n",
        "def scale_transform(data, scaler=scaler):\n",
        "  return scaler.transform(data)\n",
        "\n",
        "def pca_transform(data, pca_model=pca_model):\n",
        "  return pca_model.transform(data)\n",
        "\n",
        "def scale_pca_transform(data, scaler=scaler, pca_model=pca_model):\n",
        "  return pca_model.transform(scaler.transform(data))\n",
        "\n",
        "def normalize_scale_pca_transform(data, scaler=scaler, pca_model=pca_model):\n",
        "  return pca_model.transform(scaler.transform(normalize_data(data)))\n",
        "\n",
        "def select_colums(data, columns=deck_atributes):\n",
        "  return data.select(columns).to_numpy()"
      ],
      "metadata": {
        "id": "zD9iyjjOTZC4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Find closest clusters function"
      ],
      "metadata": {
        "id": "ROIkzYwPVUp7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def find_5_closest_clusters_of_rows (data,cluster_data, n_closest_centers=5, columns=deck_atributes):\n",
        "  normalized_data= normalize_data(data.select(columns).to_numpy())\n",
        "  # Select only the relevant columns from cluster_data for normalization\n",
        "  cluster_columns = cluster_data.select(columns).to_numpy()\n",
        "  normalized_clusters = normalize_data(cluster_columns)\n",
        "\n",
        "  # Calculate squared Euclidean distance between each data point and each cluster centroid\n",
        "  # (a-b)^2 = a^2 - 2ab + b^2\n",
        "  # sum((a-b)^2) = sum(a^2) - 2*sum(ab) + sum(b^2)\n",
        "  # sum(a^2) is the squared norm of each data point, which is 1 since data is normalized\n",
        "  # sum(b^2) is the squared norm of each cluster centroid, which is 1 since cluster centroids are normalized\n",
        "  # So squared distance is 1 - 2*sum(ab) + 1 = 2 - 2*sum(ab) = 2*(1 - sum(ab))\n",
        "  # sum(ab) is the dot product between each data point and each cluster centroid\n",
        "  # This can be calculated efficiently using matrix multiplication: normalized_data @ normalized_clusters.T\n",
        "  dot_product = normalized_data @ normalized_clusters.T\n",
        "  squared_distance = 2 * (1 - dot_product)\n",
        "\n",
        "  # Create an array with cube numbers, cluster number, and squared distance\n",
        "  # The cube numbers and cluster number are in the first 4 columns of cluster_data\n",
        "  # The squared_distance needs to be matched to the corresponding cluster\n",
        "  # We can use the index of the closest cluster\n",
        "  # This requires iterating through each row of the input data\n",
        "\n",
        "  results = []\n",
        "  # Get cube and cluster information for each cluster\n",
        "  cluster_info = cluster_data.with_row_index().select(['index','cube_1', 'cube_2', 'cube_3', 'cluster_number']).to_numpy()\n",
        "\n",
        "  for i in range(data.shape[0]):\n",
        "    # Find the index of the 5 closest cluster for the current data point\n",
        "    closest_cluster_indices = np.argpartition(squared_distance[i], 4)[:n_closest_centers]\n",
        "\n",
        "    # Get the cube and cluster number of the closest cluster\n",
        "    closest_5_array = cluster_info[closest_cluster_indices]\n",
        "\n",
        "    # Get the squared distance to the closest cluster\n",
        "    distances = squared_distance[i, closest_cluster_indices]\n",
        "\n",
        "    # join distances to closest 5 array\n",
        "    closest_5_array = np.column_stack((closest_5_array, distances))\n",
        "\n",
        "    results.append(closest_5_array)\n",
        "\n",
        "  return np.stack(results)"
      ],
      "metadata": {
        "id": "PQKqIeOYVT8-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Adjusted WR calculation fn"
      ],
      "metadata": {
        "id": "jdwNfnPxVKgH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_adjusted_win_rate_inverse_squared_weights(data,cluster_data , n_closest_centers=5, columns=deck_atributes):\n",
        "  closest_5_array = find_5_closest_clusters_of_rows(data, cluster_data, columns=columns, n_closest_centers=n_closest_centers)\n",
        "  win_rates_all = cluster_data.select('win_rate_adjusted').to_numpy()\n",
        "  total_matches_all = cluster_data.select('total_matches_adjusted').to_numpy()\n",
        "\n",
        "  weighted_win_rates = []\n",
        "  for i in range(closest_5_array.shape[0]):\n",
        "    # Get the indices of the 5 closest clusters for the current data point\n",
        "    closest_cluster_indices = closest_5_array[i, :, 0].astype(int)\n",
        "\n",
        "    # Get the squared distances to the closest clusters\n",
        "    distances_squared = closest_5_array[i, :, 5]\n",
        "\n",
        "    # Filter out clusters with zero distance to avoid division by zero\n",
        "    non_zero_distance_indices = distances_squared != 0\n",
        "    closest_cluster_indices = closest_cluster_indices[non_zero_distance_indices]\n",
        "    distances_squared = distances_squared[non_zero_distance_indices]\n",
        "\n",
        "    if len(closest_cluster_indices) == 0:\n",
        "        weighted_win_rates.append(np.nan) # Or some other indicator for no valid clusters\n",
        "        continue\n",
        "\n",
        "\n",
        "    # Get the win rates and total_matches_adjusted for the closest clusters\n",
        "    win_rates = win_rates_all[closest_cluster_indices]\n",
        "\n",
        "    total_matches = total_matches_all[closest_cluster_indices]\n",
        "\n",
        "    # Calculate the weights\n",
        "    weights = total_matches / distances_squared\n",
        "\n",
        "    # Calculate the weighted average win rate\n",
        "    weighted_average_win_rate = np.sum(win_rates * weights) / np.sum(weights)\n",
        "    weighted_win_rates.append(weighted_average_win_rate)\n",
        "\n",
        "  return np.array(weighted_win_rates)"
      ],
      "metadata": {
        "id": "kIl5GiWmizfI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VEuRdvgnVH9z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}